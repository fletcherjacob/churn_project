{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "<b>Features To Build </b>\n",
    "- Total Songs Listened To\n",
    "- App Page Interactions\n",
    "    - Thumbs Up\n",
    "    - Thumbs Down\n",
    "    - Add Friend\n",
    "    - Add to playlist\n",
    "- Help Page Interactions/Error Page Interactions\n",
    "\n",
    "\n",
    "<b> Key Points </b>\n",
    "- The Feature dataframe will be created based on the userId\n",
    "- Transformations will need to maximize Pyspark capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_psdf = psdf.dropna(subset=\"userId\")\n",
    "clean_psdf.createOrReplaceTempView(\"cleaned_user_log\")\n",
    "unique_users = clean_psdf[[\"userId\"]].distinct()\n",
    "\n",
    "clean_psdf = clean_psdf.withColumn(\"ts\", (col(\"ts\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# Apply date_format function\n",
    "clean_psdf = clean_psdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# clean_psdf = clean_psdf.withColumn(\"ts\", from_unixtime(col(\"ts\") / 1000))  # Assuming ts is in milliseconds\n",
    "# clean_psdf = psdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_users(\n",
    "    sdf: DataFrame, unique_users: DataFrame, fill_value=lit(0)\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing users in a PySpark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - sdf (DataFrame): PySpark DataFrame representing user data. Should have columns 'userId' and 'featur_name'.\n",
    "    - unique_users (DataFrame): PySpark DataFrame with unique user information.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated PySpark DataFrame with filled missing users.\n",
    "    \"\"\"\n",
    "\n",
    "    sdf_user_count = sdf.count()\n",
    "\n",
    "    unique_count = unique_users.count()\n",
    "\n",
    "    if sdf_user_count != unique_count:\n",
    "        print(f\"Missing Values: {unique_count - sdf_user_count}\")\n",
    "        missing_users = unique_users.select(\"userId\").subtract(sdf.select(\"userId\"))\n",
    "        # Since the sdf is only two we rename the column based on sdf's second column\n",
    "        missing_users_sdf = missing_users.withColumn(sdf.columns[1], fill_value)\n",
    "        filled_missing_users = sdf.union(missing_users_sdf)\n",
    "\n",
    "        return filled_missing_users\n",
    "    else:\n",
    "        return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_counts = (\n",
    "    clean_psdf[[\"userId\", \"artist\"]].dropna(subset=\"artist\").groupBy(\"userId\").count()\n",
    ")\n",
    "song_counts = song_counts.withColumnRenamed(\"count\", \"song_counts\")\n",
    "\n",
    "song_counts = handle_missing_users(song_counts, unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_artist = (\n",
    "    clean_psdf.filter(clean_psdf[\"artist\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"artist\").alias(\"distinct_artist\"))\n",
    ")\n",
    "\n",
    "distinct_artist = handle_missing_users(distinct_artist, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level = (\n",
    "    clean_psdf[[\"userId\", \"level\", \"ts\"]]\n",
    "    .orderBy(\"ts\", ascending=False)\n",
    "    .dropDuplicates(subset=[\"userId\"])\n",
    "    .select(\"userId\", \"level\")\n",
    ")\n",
    "\n",
    "level_flag_udf = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "\n",
    "# one-hot encode\n",
    "user_level = user_level.withColumn(\n",
    "    \"level_flag\", level_flag_udf(user_level[\"level\"])\n",
    ").select(\"userId\", \"level_flag\")\n",
    "\n",
    "user_level = handle_missing_users(user_level, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive App Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_usage_list = [\"Thumbs Up\", \"Thumbs Down\", \"Add Friend\", \"Add to playlist\"]\n",
    "\n",
    "positive_usage = (\n",
    "    clean_psdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(positive_usage_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "positive_usage = positive_usage.withColumnRenamed(\"count\", \"pos_interactions\")\n",
    "\n",
    "positive_usage = handle_missing_users(positive_usage, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_interactions_list = [\"Error\", \"Help\"]\n",
    "\n",
    "neg_interactions = (\n",
    "    clean_psdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(neg_interactions_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "neg_interactions = neg_interactions.withColumnRenamed(\"count\", \"neg_interactions\")\n",
    "\n",
    "\n",
    "neg_interactions = handle_missing_users(neg_interactions, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations = (\n",
    "    clean_psdf.filter(clean_psdf[\"location\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"location\").alias(\"unique_locations\"))\n",
    ")\n",
    "\n",
    "\n",
    "unique_locations = handle_missing_users(unique_locations, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Daily Listens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average daily listens per user\n",
    "average_daily_listens = (\n",
    "    clean_psdf.dropna(subset=\"artist\")\n",
    "    .groupBy(\"userId\", \"date\")\n",
    "    .agg(count(\"artist\").alias(\"daily_listens\"))\n",
    ")\n",
    "average_daily_listens = average_daily_listens.groupBy(\"userId\").agg(\n",
    "    {\"daily_listens\": \"avg\"}\n",
    ")\n",
    "\n",
    "average_daily_listens = handle_missing_users(average_daily_listens, unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_filter = [\"Cancel\", \"Cancellation Confirmation\", \"NextSong\"]\n",
    "page_count_df = (\n",
    "    clean_psdf[[\"userId\", \"page\"]]\n",
    "    .filter(~col(\"page\").isin(page_filter))\n",
    "    .toPandas()\n",
    "    .groupby(\"userId\")\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"page\", values=\"count\", index=\"userId\")\n",
    "    .fillna(int(0))\n",
    ")\n",
    "\n",
    "\n",
    "page_count_corr = page_count_df.corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    page_count_corr,\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "\n",
    "page_count = spark.createDataFrame(page_count_df.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "clean_psdf.groupBy(\"userId\").agg(\n",
    "    countDistinct(\"sessionId\").alias(\"sessionCount\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_psdf[[\"userId\", \"page\"]].filter(\n",
    "    col(\"page\").isin([\"Cancellation Confirmation\"])\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\n",
    "    clean_psdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin([\"Cancellation Confirmation\", \"Cancel\"]))\n",
    "    .drop_duplicates([\"userId\"])\n",
    ")\n",
    "\n",
    "\n",
    "labels = labels.withColumn(\"label\", lit(1))\n",
    "labels_df = labels.drop(\"page\")\n",
    "\n",
    "labels_df = handle_missing_users(labels_df, unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\n",
    "    clean_psdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin([\"Cancellation Confirmation\", \"Cancel\"]))\n",
    "    .drop_duplicates([\"userId\"])\n",
    ")\n",
    "\n",
    "\n",
    "labels = labels.withColumn(\"label\", lit(1))\n",
    "labels_df = labels.drop(\"page\")\n",
    "\n",
    "labels_df = handle_missing_users(labels_df, unique_users)\n",
    "\n",
    "labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    # song_counts,\n",
    "    # song_listened_mean,\n",
    "    average_daily_listens,\n",
    "    user_level,\n",
    "    positive_usage,\n",
    "    neg_interactions,\n",
    "    unique_locations,\n",
    "    distinct_artist,\n",
    "    page_count,\n",
    "]\n",
    "\n",
    "joined_features = labels_df\n",
    "for df in dfs:\n",
    "    joined_features = joined_features.join(df, \"userId\", \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Features For Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_features.dropDuplicates().drop(\"userId\").toPandas().to_csv(\n",
    "    \"mini_selected_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_features_df = joined_features.toPandas().drop(labels=\"userId\", axis=1)\n",
    "# joined_features_df = joined_features_df.drop(\n",
    "#     labels=[\"song_counts\", \"unique_locations\"], axis=1\n",
    "# )\n",
    "\n",
    "correlation_matrix = joined_features_df.corr()\n",
    "\n",
    "joined_features_df\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Extracted Feature Correalations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Filter correlations greater than 0.8 (adjust the threshold as needed)\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "# Iterate over all combinations of column pairs\n",
    "for col1, col2 in combinations(correlation_matrix.columns, 2):\n",
    "    if correlation_matrix[col1][col2] > 0.8:\n",
    "        highly_correlated_pairs.append((col1, col2, correlation_matrix[col1][col2]))\n",
    "\n",
    "# Print or use the filtered pairs\n",
    "print(\"Highly Correlated Pairs:\")\n",
    "\n",
    "\n",
    "print(pd.DataFrame(data=highly_correlated_pairs).sort_values(by=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = joined_features.toPandas()\n",
    "\n",
    "for column in pandas_df.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=pandas_df[column])\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = joined_features_df.drop_duplicates().fillna(0).copy()\n",
    "\n",
    "Y = df[\"label\"]\n",
    "X = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "pca = PCA()\n",
    "X = pca.fit_transform(X)\n",
    "pca.get_covariance()\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(\"dark_background\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(\n",
    "        range(len(explained_variance)),\n",
    "        explained_variance,\n",
    "        alpha=0.5,\n",
    "        align=\"center\",\n",
    "        label=\"individual explained variance\",\n",
    "    )\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.ylabel(\"Explained variance ratio\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(\"dark_background\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(\n",
    "        range(6), explained_variance[:6], alpha=0.5, align=\"center\"\n",
    "    )  # , label='individual explained variance' )\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.ylabel(\"Explained variance ratio\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_psdf[[\"location\", \"userId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 12\n",
    "str(ex) + \"g\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
