{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "<b>Features To Build </b>\n",
    "- Total Songs Listened To\n",
    "- App Page Interactions\n",
    "    - Thumbs Up\n",
    "    - Thumbs Down\n",
    "    - Add Friend\n",
    "    - Add to playlist\n",
    "- Help Page Interactions/Error Page Interactions\n",
    "- User Device Brand\n",
    "- User Browser\n",
    "\n",
    "\n",
    "<b> Key Points </b>\n",
    "- The Feature dataframe will be created based on the userId\n",
    "- Transformations will need to maximize Pyspark capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, from_unixtime, date_trunc, udf, lit,date_format\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local variable 'spark' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/20 23:37:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session(app_name=\"Sparkify\", default_settings = True ,  total_physical_cores=16,driver_memory = 8,executor_memory = 8):\n",
    "# Calculate available cores for Spark\n",
    "    try:\n",
    "        spark.shutdown()\n",
    "    except  Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if default_settings == False:\n",
    "        total_physical_cores = input(\" Available Cores\")\n",
    "        driver_memory =  input(\" Driver Memory Allowance\")\n",
    "        executor_memory = input(\"Executor Memory Allowance\")\n",
    "\n",
    "    available_cores_for_spark =int( total_physical_cores - 2)\n",
    "    # Configure Spark session\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.driver.memory\", str(int(driver_memory)) + \"g\")\n",
    "        .config(\"spark.executor.memory\", str(int(executor_memory)) + \"g\")\n",
    "        .config(\"spark.executor.cores\", available_cores_for_spark)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return  spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"./data/lg_sparkify_event_data.json\"\n",
    "sdf = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_sdf = sdf.dropna(subset=\"userId\")\n",
    "clean_sdf.createOrReplaceTempView(\"cleaned_user_log\")\n",
    "unique_users = clean_sdf[[\"userId\"]].distinct()\n",
    "\n",
    "clean_sdf = clean_sdf.withColumn(\"ts\", (col(\"ts\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# Apply date_format function\n",
    "clean_sdf = clean_sdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# clean_sdf = clean_sdf.withColumn(\"ts\", from_unixtime(col(\"ts\") / 1000))  # Assuming ts is in milliseconds\n",
    "# clean_sdf = sdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_users(\n",
    "    sdf: DataFrame, unique_users: DataFrame, fill_value=lit(0)\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing users in a PySpark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - sdf (DataFrame): PySpark DataFrame representing user data. Should have columns 'userId' and 'featur_name'.\n",
    "    - unique_users (DataFrame): PySpark DataFrame with unique user information.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated PySpark DataFrame with filled missing users.\n",
    "    \"\"\"\n",
    "\n",
    "    sdf_user_count = sdf.count()\n",
    "\n",
    "    unique_count = unique_users.count()\n",
    "\n",
    "    if sdf_user_count != unique_count:\n",
    "        print(f\"Missing Values: {unique_count - sdf_user_count}\")\n",
    "        missing_users = unique_users.select(\"userId\").subtract(sdf.select(\"userId\"))\n",
    "        # Since the sdf is only two we rename the column based on sdf's second column\n",
    "        missing_users_sdf = missing_users.withColumn(sdf.columns[1], fill_value)\n",
    "        filled_missing_users = sdf.union(missing_users_sdf)\n",
    "\n",
    "        return filled_missing_users\n",
    "    else:\n",
    "        return sdf\n",
    "    \n",
    "def flag_rows(df, column_name, check_list, flagged_column_name):\n",
    "    \"\"\"\n",
    "    Flag rows in a PySpark DataFrame based on whether the value in a specified column is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pyspark.sql.DataFrame): The PySpark DataFrame to be modified.\n",
    "    - column_name (str): The name of the column to check for values.\n",
    "    - check_list (list): The list of values to check against.\n",
    "    - flagged_column_name (str): The name of the new column to be created for the flags.\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.DataFrame: The modified PySpark DataFrame with the new flagged column.\n",
    "\n",
    "    This function takes a PySpark DataFrame, a column name, a list of values, and a flagged column name. It then adds a new column to the DataFrame\n",
    "    that contains a flag (1 or 0) based on whether the values in the specified column are present in the given list.\n",
    "    \"\"\"\n",
    "\n",
    "    def check_list_udf(value):\n",
    "        return 1 if value in check_list else 0\n",
    "\n",
    "    check_list_udf = udf(check_list_udf, IntegerType())\n",
    "\n",
    "    df = df.withColumn(flagged_column_name, check_list_udf(df[column_name]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "song_counts = (\n",
    "    clean_sdf[[\"userId\", \"artist\"]].dropna(subset=\"artist\").groupBy(\"userId\").count()\n",
    ")\n",
    "\n",
    "song_counts = handle_missing_users(song_counts, unique_users)\n",
    "\n",
    "# song_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17\n"
     ]
    }
   ],
   "source": [
    "distinct_artist = (\n",
    "    clean_sdf.filter(clean_sdf[\"artist\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"artist\").alias(\"distinct_artist\"))\n",
    ")\n",
    "\n",
    "distinct_artist = handle_missing_users(distinct_artist, unique_users)\n",
    "\n",
    "# distinct_artist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_level = (\n",
    "    clean_sdf[[\"userId\", \"level\", \"ts\"]]\n",
    "    .orderBy(\"ts\", ascending=False)\n",
    "    .dropDuplicates(subset=[\"userId\"])\n",
    "    .select(\"userId\", \"level\")\n",
    ")\n",
    "\n",
    "level_flag_udf = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "\n",
    "# one-hot encode\n",
    "user_level = user_level.withColumn(\n",
    "    \"level_flag\", level_flag_udf(user_level[\"level\"])\n",
    ").select(\"userId\", \"level_flag\")\n",
    "\n",
    "user_level = handle_missing_users(user_level, unique_users)\n",
    "# user_level.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive App Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_usage_list = [\"Thumbs Up\", \"Thumbs Down\", \"Add Friend\", \"Add to playlist\"]\n",
    "\n",
    "positive_usage = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(positive_usage_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "positive_usage = positive_usage.withColumnRenamed(\"count\", \"pos_interactions\")\n",
    "\n",
    "positive_usage = handle_missing_users(positive_usage, unique_users)\n",
    "# positive_usage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 3412\n"
     ]
    }
   ],
   "source": [
    "neg_interactions_list = [\"Error\", \"Help\"]\n",
    "\n",
    "neg_interactions = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(neg_interactions_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "neg_interactions = neg_interactions.withColumnRenamed(\"count\", \"neg_interactions\")\n",
    "\n",
    "neg_interactions = handle_missing_users(neg_interactions, unique_users)\n",
    "# neg_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:================================================>      (85 + 11) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_locations = (\n",
    "    clean_sdf.filter(clean_sdf[\"location\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"location\").alias(\"unique_locations\"))\n",
    ")\n",
    "\n",
    "\n",
    "unique_locations = handle_missing_users(unique_locations, unique_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Daily Listens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average daily listens per user\n",
    "average_daily_listens = (\n",
    "    clean_sdf.dropna(subset=\"artist\")\n",
    "    .groupBy(\"userId\", \"date\")\n",
    "    .agg(F.count(\"artist\").alias(\"daily_listens\"))\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.avg(\"daily_listens\").alias(\"avg_daily_listens\"))\n",
    ")\n",
    "\n",
    "average_daily_listens = average_daily_listens.withColumn(\"avg_daily_listens\", F.round(\"avg_daily_listens\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "page_filter = [\"Cancel\", \"Cancellation Confirmation\", \"NextSong\"]\n",
    "page_count_df = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(~col(\"page\").isin(page_filter))\n",
    "    .toPandas()\n",
    "    .groupby(\"userId\")\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"page\", values=\"count\", index=\"userId\")\n",
    "    .fillna(int(0))\n",
    ")\n",
    "\n",
    "\n",
    "# page_count_corr = page_count_df.corr()\n",
    "\n",
    "# # sns.heatmap(\n",
    "# #     page_count_corr,\n",
    "# #     annot=False,\n",
    "# #     cmap=\"coolwarm\",\n",
    "# #     fmt=\".2f\",\n",
    "# #     linewidths=0.5,\n",
    "# # )\n",
    "\n",
    "page_count = spark.createDataFrame(page_count_df.reset_index())\n",
    "# page_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and Browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ua_check(sdf):\n",
    "    from ua_parser import user_agent_parser\n",
    "\n",
    "    def device_check(ua):\n",
    "        if ua:\n",
    "            ua = ua.replace(\"\\\\\", \"\")\n",
    "            parsed_string = user_agent_parser.Parse(ua)\n",
    "            return parsed_string[\"os\"][\"family\"]\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    def browser_check(ua):\n",
    "        if ua:\n",
    "            ua = ua.replace(\"\\\\\", \"\")\n",
    "            parsed_string = user_agent_parser.Parse(ua)\n",
    "            return parsed_string[\"user_agent\"][\"family\"]\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    device_check_udf = udf(device_check, StringType())\n",
    "    browse_brand_udf = udf(browser_check, StringType())\n",
    "\n",
    "    sdf = sdf.withColumn(\"device\", device_check_udf(sdf[\"userAgent\"]))\n",
    "    sdf = sdf.withColumn(\"browser\", browse_brand_udf(sdf[\"userAgent\"]))\n",
    "\n",
    "    return sdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "cols_to_encode = [\"device\",\"browser\"]\n",
    "\n",
    "def one_hot_encode(sdf,cols_to_encode):\n",
    "    conditions = {}\n",
    "    categories = {}\n",
    "\n",
    "    for col in cols_to_encode:\n",
    "        cats = [row[0] for row in sdf.select(col).distinct().collect()]\n",
    "        conditions[col] = {f\"{col} == '{cat}'\": idx for idx, cat in enumerate(cats)}\n",
    "        categories[col] = {cat: idx for idx, cat in enumerate(cats)}\n",
    "\n",
    "    for col, cats_dict in categories.items():\n",
    "        for cat, idx in cats_dict.items():\n",
    "            expression = F.when(sdf[col] == cat, 1).otherwise(0)\n",
    "            sdf = sdf.withColumn(f\"{col}_{cat}\", expression)\n",
    "        sdf = sdf.drop(col)\n",
    "\n",
    "    return sdf\n",
    "\n",
    "ua_sdf = one_hot_encode(ua_check(sdf).select(\"userId\",\"device\",\"browser\"),cols_to_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Since Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_registration = sdf.groupBy('userId', 'ts', 'registration').count() \\\n",
    "    .withColumn('life_time', (F.col('ts') - F.col('registration')) / 1000) \\\n",
    "    .groupBy('userId').agg(F.max('life_time').alias('time_since_registration'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:================================================>      (85 + 11) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin([\"Cancellation Confirmation\"]))\n",
    "    .drop_duplicates([\"userId\"])\n",
    ")\n",
    "\n",
    "\n",
    "labels = labels.withColumn(\"label\", lit(1))\n",
    "labels_sdf = labels.drop(\"page\")\n",
    "\n",
    "labels_sdf = handle_missing_users(labels_sdf, unique_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    song_counts,\n",
    "    # song_listened_mean,\n",
    "    average_daily_listens,\n",
    "    user_level,\n",
    "    positive_usage,\n",
    "    neg_interactions,\n",
    "    #unique_locations,\n",
    "    distinct_artist,\n",
    "    page_count,\n",
    "    time_since_registration,\n",
    "   ua_sdf\n",
    "]\n",
    "\n",
    "processed_features = labels_sdf\n",
    "for df in dfs:\n",
    "    processed_features = processed_features.join(df, \"userId\", \"left_outer\")\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features = processed_features.dropDuplicates().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Features For Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/20 23:59:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "processed_features.toPandas().to_csv(f\"./data/lg_all_features.csv\")\n",
    "                                \n",
    "spark.stop()        \n",
    "                                \n",
    "#processed_features.write.csv(f\"/Users/jacobfletcher/git/churn_project/data/processed_features_{datetime.now()}\", mode='overwrite',header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_df = processed_features.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distributions(df):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    num_numerical_columns = len(numerical_columns)\n",
    "    \n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_rows = (num_numerical_columns - 1) // 3 + 1\n",
    "    num_cols = min(3, num_numerical_columns)\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "    for i, column in enumerate(numerical_columns, start=1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        sns.histplot(df[column], kde=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "display_distributions(pf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkify_churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
