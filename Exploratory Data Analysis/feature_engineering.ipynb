{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "<b>Features To Build </b>\n",
    "- Total Songs Listened To\n",
    "- App Page Interactions\n",
    "    - Thumbs Up\n",
    "    - Thumbs Down\n",
    "    - Add Friend\n",
    "    - Add to playlist\n",
    "- Help Page Interactions/Error Page Interactions\n",
    "- User Device Brand\n",
    "- User Browser\n",
    "\n",
    "\n",
    "<b> Key Points </b>\n",
    "- The Feature dataframe will be created based on the userId\n",
    "- Transformations will need to maximize Pyspark capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, StringType,DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, from_unixtime, date_trunc, udf, lit,date_format\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local variable 'spark' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 23:25:32 WARN Utils: Your hostname, Jacobs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.156 instead (on interface en0)\n",
      "24/01/30 23:25:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/30 23:25:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/30 23:25:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session(app_name=\"Sparkify\", default_settings = True ,  total_physical_cores=16,driver_memory = 8,executor_memory = 8):\n",
    "# Calculate available cores for Spark\n",
    "    try:\n",
    "        spark.shutdown()\n",
    "    except  Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if default_settings == False:\n",
    "        total_physical_cores = input(\" Available Cores\")\n",
    "        driver_memory =  input(\" Driver Memory Allowance\")\n",
    "        executor_memory = input(\"Executor Memory Allowance\")\n",
    "\n",
    "    available_cores_for_spark =int( total_physical_cores - 2)\n",
    "    # Configure Spark session\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.driver.memory\", str(int(driver_memory)) + \"g\")\n",
    "        .config(\"spark.executor.memory\", str(int(executor_memory)) + \"g\")\n",
    "        .config(\"spark.executor.cores\", available_cores_for_spark)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return  spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"../data/lg_sparkify_event_data.json\"\n",
    "sdf = spark.read.json(path)\n",
    "display = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_sdf = sdf.dropna(subset=\"userId\")\n",
    "clean_sdf.createOrReplaceTempView(\"cleaned_user_log\")\n",
    "unique_users = clean_sdf[[\"userId\"]].distinct()\n",
    "\n",
    "clean_sdf = clean_sdf.withColumn(\"ts\", (col(\"ts\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# Apply date_format function\n",
    "clean_sdf = clean_sdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# clean_sdf = clean_scopy_df.withColumn(\"ts\", from_unixtime(col(\"ts\") / 1000))  # Assuming ts is in milliseconds\n",
    "# clean_sdf = sdf.withColumn(\"date\", date_format(col(\"ts\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_users(\n",
    "    sdf: DataFrame, unique_users: DataFrame, fill_value=lit(0)\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing users in a PySpark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - sdf (DataFrame): PySpark DataFrame representing user data. Should have columns 'userId' and 'featur_name'.\n",
    "    - unique_users (DataFrame): PySpark DataFrame with unique user information.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated PySpark DataFrame with filled missing users.\n",
    "    \"\"\"\n",
    "\n",
    "    sdf_user_count = sdf.count()\n",
    "\n",
    "    unique_count = unique_users.count()\n",
    "\n",
    "    if sdf_user_count != unique_count:\n",
    "        print(f\"Missing Values: {unique_count - sdf_user_count}\")\n",
    "        missing_users = unique_users.select(\"userId\").subtract(sdf.select(\"userId\"))\n",
    "        # Since the sdf is only two we rename the column based on sdf's second column\n",
    "        missing_users_sdf = missing_users.withColumn(sdf.columns[1], fill_value)\n",
    "        filled_missing_users = sdf.union(missing_users_sdf)\n",
    "\n",
    "        return filled_missing_users\n",
    "    else:\n",
    "        return sdf\n",
    "    \n",
    "def flag_rows(df, column_name, check_list, flagged_column_name):\n",
    "    \"\"\"\n",
    "    Flag rows in a PySpark DataFrame based on whether the value in a specified column is in a given list.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pyspark.sql.DataFrame): The PySpark DataFrame to be modified.\n",
    "    - column_name (str): The name of the column to check for values.\n",
    "    - check_list (list): The list of values to check against.\n",
    "    - flagged_column_name (str): The name of the new column to be created for the flags.\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.DataFrame: The modified PySpark DataFrame with the new flagged column.\n",
    "\n",
    "    This function takes a PySpark DataFrame, a column name, a list of values, and a flagged column name. It then adds a new column to the DataFrame\n",
    "    that contains a flag (1 or 0) based on whether the values in the specified column are present in the given list.\n",
    "    \"\"\"\n",
    "\n",
    "    def check_list_udf(value):\n",
    "        return 1 if value in check_list else 0\n",
    "\n",
    "    check_list_udf = udf(check_list_udf, IntegerType())\n",
    "\n",
    "    df = df.withColumn(flagged_column_name, check_list_udf(df[column_name]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin([\"Cancellation Confirmation\"]))\n",
    "    .drop_duplicates([\"userId\"])\n",
    ")\n",
    "\n",
    "\n",
    "labels = labels.withColumn(\"label\", lit(1))\n",
    "labels = labels.drop(\"page\")\n",
    "\n",
    "labels = handle_missing_users(labels, unique_users)\n",
    "\n",
    "features_sdf = labels\n",
    "\n",
    "if display == True:\n",
    "    #Plot Counts\n",
    "    df = labels.toPandas().groupby(by=\"label\").count().reset_index()\n",
    "    sns.barplot(data = df, x = \"label\", y = \"userId\", hue = \"label\")\n",
    "    df.groupby(\"label\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17\n"
     ]
    }
   ],
   "source": [
    "song_counts = (\n",
    "    clean_sdf[[\"userId\", \"artist\"]].dropna(subset=\"artist\").groupBy(\"userId\").count()\n",
    ")\n",
    "\n",
    "song_counts =song_counts.withColumnRenamed(\"count\",\"song_counts\")\n",
    "song_counts = handle_missing_users(song_counts, unique_users)\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(song_counts, \"userId\", \"left_outer\").select(\"label\",\"song_counts\").toPandas()\n",
    "    avg_df = df.groupby(\"label\").agg({'song_counts': 'mean'}).rename(columns={'song_counts': 'avg_song_counts'}).round()\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"avg_song_counts\", hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distinct_artist = (\n",
    "    clean_sdf.filter(clean_sdf[\"artist\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"artist\").alias(\"distinct_artist\"))\n",
    ")\n",
    "\n",
    "distinct_artist = handle_missing_users(distinct_artist, unique_users)\n",
    "if display == True:\n",
    "    df= labels.join(distinct_artist, \"userId\", \"left_outer\").select(\"label\",\"distinct_artist\").toPandas()\n",
    "    avg_df = df.groupby(\"label\").agg({'distinct_artist': 'mean'}).rename(columns={'distinct_artist': 'avg_distinct_artist'}).round()\n",
    "\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"avg_distinct_artist\", hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_level = (\n",
    "    clean_sdf[[\"userId\", \"level\", \"ts\"]]\n",
    "    .orderBy(\"ts\", ascending=False)\n",
    "    .dropDuplicates(subset=[\"userId\"])\n",
    "    .select(\"userId\", \"level\")\n",
    ")\n",
    "\n",
    "level_flag_udf = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "# one-hot encode\n",
    "user_level = user_level.withColumn(\n",
    "    \"level_flag\", level_flag_udf(user_level[\"level\"])\n",
    ").select(\"userId\", \"level_flag\")\n",
    "\n",
    "user_level = handle_missing_users(user_level, unique_users)\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(user_level, \"userId\", \"left_outer\").select(\"label\",\"level_flag\").toPandas()\n",
    "    df= df.groupby(\"label\").value_counts().reset_index()\n",
    "    sns.barplot(data = df,x=\"level_flag\",y=\"count\",hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive App Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_usage_list = [\"Thumbs Up\", \"Thumbs Down\", \"Add Friend\", \"Add to playlist\"]\n",
    "\n",
    "positive_usage = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(positive_usage_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "positive_usage = positive_usage.withColumnRenamed(\"count\", \"pos_interactions\")\n",
    "\n",
    "positive_usage = handle_missing_users(positive_usage, unique_users)\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(positive_usage, \"userId\", \"left_outer\").select(\"label\",\"pos_interactions\").toPandas()\n",
    "    avg_df = df.groupby(\"label\").agg({'pos_interactions': 'mean'}).round()\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"pos_interactions\", hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "neg_interactions_list = [\"Error\", \"Help\"]\n",
    "\n",
    "neg_interactions = (\n",
    "    clean_sdf[[\"userId\", \"page\"]]\n",
    "    .filter(col(\"page\").isin(neg_interactions_list))\n",
    "    .groupBy(\"userId\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "\n",
    "#Display\n",
    "neg_interactions = neg_interactions.withColumnRenamed(\"count\", \"neg_interactions\")\n",
    "neg_interactions = handle_missing_users(neg_interactions, unique_users)\n",
    "\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(neg_interactions, \"userId\", \"left_outer\").select(\"label\",\"neg_interactions\").toPandas()\n",
    "    avg_df = df.groupby(\"label\").agg({'neg_interactions': 'mean'}).round()\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"neg_interactions\", hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=================================================>     (86 + 10) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_locations = (\n",
    "    clean_sdf.filter(clean_sdf[\"location\"].isNotNull())\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.countDistinct(\"location\").alias(\"unique_locations\"))\n",
    ")\n",
    "\n",
    "\n",
    "unique_locations = handle_missing_users(unique_locations, unique_users)\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    #Display\n",
    "    df= labels.join(unique_locations, \"userId\", \"left_outer\").select(\"label\",\"unique_locations\").toPandas()\n",
    "    avg_df = df.groupby(\"label\").agg({'unique_locations': 'mean'}).round()\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"unique_locations\", hue = \"label\")\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Daily Listens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average daily listens per user\n",
    "average_daily_listens = (\n",
    "    clean_sdf.dropna(subset=\"artist\")\n",
    "    .groupBy(\"userId\", \"date\")\n",
    "    .agg(F.count(\"artist\").alias(\"daily_listens\"))\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(F.avg(\"daily_listens\").alias(\"avg_daily_listens\"))\n",
    ")\n",
    "\n",
    "average_daily_listens = average_daily_listens.withColumn(\"avg_daily_listens\", F.round(\"avg_daily_listens\"))\n",
    "\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    #Display\n",
    "    df= labels.join(average_daily_listens, \"userId\", \"left_outer\").select(\"label\",\"avg_daily_listens\").toPandas()\n",
    "\n",
    "    avg_df = df.groupby(\"label\").agg({'avg_daily_listens': 'mean'}).round()\n",
    "    sns.barplot(data = avg_df, x = \"label\", y = \"avg_daily_listens\", hue = \"label\")\n",
    "\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "page_filter = [\"Cancel\", \"Cancellation Confirmation\", \"NextSong\"]\n",
    "filtered_sdf = clean_sdf.select(\"userId\", \"page\").filter(~F.col(\"page\").isin(page_filter))\n",
    "\n",
    "# Grouping and Counting\n",
    "page_count_sdf = (\n",
    "filtered_sdf\n",
    "    .groupBy(\"userId\", \"page\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "page_count_sdf=page_count_sdf.groupBy(\"userId\").pivot('page').agg(F.sum(\"count\")).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(page_count_sdf, \"userId\", \"left_outer\").toPandas()\n",
    "    df = df.loc[:, df.columns != \"userId\"].groupby(\"label\").mean()\n",
    "    num_columns = len(df.columns)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_columns, figsize=(15, 6), sharey=True)\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "        df[[col]].T.plot(kind='bar', stacked=True, ax=axes[i], legend=False)\n",
    "        axes[i].set_ylabel(\"frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(title='Columns', bbox_to_anchor=(1.05, 1), loc='upper left',  labels=\"label\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ua_parser in /Users/jacobfletcher/anaconda3/envs/sparkify_churn/lib/python3.10/site-packages (0.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ua_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and Browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ua_check(sdf):\n",
    "    from ua_parser import user_agent_parser\n",
    "\n",
    "    def device_check(ua):\n",
    "        if ua:\n",
    "            ua = ua.replace(\"\\\\\", \"\")\n",
    "            parsed_string = user_agent_parser.Parse(ua)\n",
    "            return parsed_string[\"os\"][\"family\"]\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    def browser_check(ua):\n",
    "        if ua:\n",
    "            ua = ua.replace(\"\\\\\", \"\")\n",
    "            parsed_string = user_agent_parser.Parse(ua)\n",
    "            return parsed_string[\"user_agent\"][\"family\"]\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    device_check_udf = udf(device_check, StringType())\n",
    "    browse_brand_udf = udf(browser_check, StringType())\n",
    "\n",
    "    sdf = sdf.withColumn(\"device\", device_check_udf(sdf[\"userAgent\"]))\n",
    "    sdf = sdf.withColumn(\"browser\", browse_brand_udf(sdf[\"userAgent\"]))\n",
    "\n",
    "    return sdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "cols_to_encode = [\"device\",\"browser\"]\n",
    "\n",
    "def one_hot_encode(sdf,cols_to_encode):\n",
    "    conditions = {}\n",
    "    categories = {}\n",
    "\n",
    "    for col in cols_to_encode:\n",
    "        cats = [row[0] for row in sdf.select(col).distinct().collect()]\n",
    "        conditions[col] = {f\"{col} == '{cat}'\": idx for idx, cat in enumerate(cats)}\n",
    "        categories[col] = {cat: idx for idx, cat in enumerate(cats)}\n",
    "\n",
    "    for col, cats_dict in categories.items():\n",
    "        for cat, idx in cats_dict.items():\n",
    "            expression = F.when(sdf[col] == cat, 1).otherwise(0)\n",
    "            sdf = sdf.withColumn(f\"{col}_{cat}\", expression)\n",
    "        sdf = sdf.drop(col)\n",
    "\n",
    "    return sdf\n",
    "\n",
    "ua_sdf = one_hot_encode(ua_check(sdf).select(\"userId\",\"device\",\"browser\"),cols_to_encode)\n",
    "\n",
    "\n",
    "# Using the max() function retains whether a user at anytime used multiple device/browsers\n",
    "columns_to_max = [col for col in ua_sdf.columns if col != 'userId']\n",
    "# Loop allows feature to retain original name\n",
    "ua_sdf = ua_sdf.groupby(\"userId\").agg(*[F.max(col).alias(col) for col in columns_to_max])\n",
    "\n",
    "if display == True:\n",
    "    #Display\n",
    "    df= labels.join(ua_sdf, \"userId\", \"left_outer\").toPandas()\n",
    "\n",
    "    df = df.loc[:, df.columns != \"userId\"].groupby(\"label\").sum()\n",
    "    num_columns = len(df.columns)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_columns, figsize=(15, 6), sharey=True)\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "        df[[col]].T.plot(kind='bar', stacked=True, ax=axes[i], legend=False)\n",
    "        axes[i].set_ylabel(\"frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(title='Columns', bbox_to_anchor=(1.05, 1), loc='upper left', labels=\"label\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Since Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_registration = sdf.groupBy('userId', 'ts', 'registration').count() \\\n",
    "    .withColumn('life_time', ((F.col('ts') - F.col('registration'))/ 86400000)) \\\n",
    "    .groupBy('userId').agg(F.round((F.max('life_time'))).alias('days_since_registration'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if display == True:\n",
    "    df= labels.join(time_since_registration, \"userId\", \"left_outer\").toPandas()\n",
    "\n",
    "    df_label_1 = df[df[\"label\"] == 1].describe().drop(columns=\"label\")\n",
    "    df_label_0 = df[df[\"label\"] == 0].describe().drop(columns=\"label\")\n",
    "    merged_df = pd.merge(df_label_1, df_label_0, left_index=True, right_index=True, suffixes=('_1', '_0'))\n",
    "    merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg. Day Between Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming you have a SparkSession named 'spark' and a DataFrame named 'your_data'\n",
    "# Replace 'your_data' with the actual DataFrame containing the columns 'userId', 'sessionId', and 'ts'\n",
    "\n",
    "# Assuming 'ts' is in timestamp format, otherwise, you may need to convert it.\n",
    "\n",
    "# Step 1: Calculate time differences\n",
    "window_spec = Window.partitionBy(\"userId\", \"sessionId\").orderBy(\"ts\")\n",
    "\n",
    "session_duration = sdf.withColumn(\"session_duration\",(F.col(\"ts\") - F.lag(\"ts\").over(window_spec))/ 10000)\n",
    "\n",
    "# Step 2: Aggregate to find the average time between sessions for each user\n",
    "average_time_between_sessions = session_duration.groupBy(\"userId\").agg(\n",
    "     F.round(F.avg(\"session_duration\")).alias(\"avg_time_between_sessions\")\n",
    ")\n",
    "\n",
    "\n",
    "if display == True:\n",
    "     # Show the result\n",
    "     average_daily_time_between_sessions = handle_missing_users(average_time_between_sessions, unique_users)\n",
    "\n",
    "     average_daily_time_between_sessions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_sessions = sdf.select(\"userId\",\"sessionId\").groupBy(\"userId\").agg(\n",
    "    F.countDistinct(\"*\").alias(\"unique_sessions\")\n",
    ")\n",
    "\n",
    "unique_sessions = handle_missing_users(unique_sessions,unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sdfs= [\n",
    "    song_counts,\n",
    "    average_daily_listens,\n",
    "    user_level,\n",
    "    positive_usage,\n",
    "    neg_interactions,\n",
    "    # unique_locations,\n",
    "    page_count_sdf,\n",
    "    ua_sdf,\n",
    "    distinct_artist,\n",
    "    time_since_registration,\n",
    "    unique_sessions,\n",
    "]\n",
    "\n",
    "processed_features = labels\n",
    "from pyspark.sql.functions import log1p, col\n",
    "\n",
    "\n",
    "def build_features(processed_sdf, feature_sdfs, use_log):\n",
    "    for sdf in feature_sdfs:\n",
    "        if use_log == True:\n",
    "            distinct_count = sdf.select(sdf.columns[1]).distinct().count()\n",
    "            if distinct_count > 2:\n",
    "                sdf = sdf.withColumn(sdf.columns[1], F.log1p(col(sdf.columns[1])))\n",
    "            \n",
    "        processed_sdf = processed_sdf.join(sdf, \"userId\", \"left_outer\")\n",
    "\n",
    "    return processed_sdf.dropDuplicates().dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Features For Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 23:32:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = build_features(labels,feature_sdfs,use_log= True)\n",
    "df1.toPandas().to_csv(\"./lg_log4_all_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = build_features(labels,feature_sdfs,use_log= False)\n",
    "df2.toPandas().to_csv(\"./lg_noLog_all_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../data/lg_log_all_features.csv/part-00000-711a9c3b-a5f1-4956-8a38-8a3ab92fde37-c000.csv\")\n",
    "# df.rename(columns={\"count\":\"song_counts\"},inplace = True)\n",
    "# df = df.drop(columns=['Unnamed: 0', 'userId'])\n",
    "\n",
    "\n",
    "df = log_processed_feature.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() > 2:\n",
    "        data = df[[col]].value_counts().reset_index()\n",
    "        sns.scatterplot(data=data,x= col,y = \"count\")# ,bins=round(data.shape[0]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "\n",
    "# df_corr = df_corr[df_corr >.8]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_corr, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Extracted Feature Correalations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distributions(df):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    numerical_columns = [col for col in df.columns if df[col].nunique() > 2]\n",
    "    num_numerical_columns = len(numerical_columns)\n",
    "    \n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_rows = (num_numerical_columns - 1) // 3 + 1\n",
    "    num_cols = min(3, num_numerical_columns)\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "    for i, column in enumerate(numerical_columns, start=1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        sns.histplot(df[column], kde=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_and_clip(df, display =False):\n",
    "    feat_list = [col for col in df.columns]\n",
    "    try:\n",
    "        feat_list.remove('userId')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def soft_clip(x, threshold=3,tolerance=1e-10):\n",
    "        mask = np.isclose(x, 0, atol=0)\n",
    "        return np.where(mask, x, np.tanh(x / threshold) * threshold)\n",
    "    \n",
    "\n",
    "    for col in df[feat_list]:\n",
    "        if df[col].nunique() :\n",
    "            # Clip and log for original columns\n",
    "            df[col + '_clipped'] = df[col].clip(lower=df[col].quantile(0.01), upper=df[col].quantile(0.99))\n",
    "            df[col + '_log'] = np.log1p(df[col])\n",
    "            # df[col + '_softClip'] = soft_clip(df[col])\n",
    "\n",
    "    for col in df[feat_list].columns:\n",
    "        if col.endswith('_log'):\n",
    "            if df[col].nunique() > 2:\n",
    "                df[col + '_logClip'] = df[col].clip(lower=df[col].quantile(0.01), upper=df[col].quantile(0.99))\n",
    "\n",
    "\n",
    "\n",
    "    if display == True:\n",
    "        for val in df.columns:\n",
    "            feat_columns = [col for col in df.columns if col.startswith(val)]\n",
    "            if len(feat_columns) > 1:\n",
    "                display_distributions(df[feat_columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "log_and_clip(df1.toPandas()[['song_counts']],True)\n",
    "log_and_clip(df2.toPandas()[['song_counts']],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skew(df,display=True):\n",
    "    skew_df =df.copy()\n",
    "    skew ={}\n",
    "    for col in df.columns:\n",
    "        skew[col+ \"_skew\"] = df[col].skew()\n",
    "    skew_df = pd.DataFrame(data=[skew.values()], columns=skew.keys()).transpose().sort_values(by = 0 , ascending=False)\n",
    "    if display:\n",
    "        skew_df.plot(kind= \"bar\")\n",
    "\n",
    "    return skew_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_skew(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_log_df = log_and_clip(df,False) \n",
    "log_cols = [col for col in clip_log_df.columns if col.endswith(\"_log\")]\n",
    "clip_cols = [col for col in clip_log_df.columns if col.endswith(\"_clipped\")]\n",
    "clipLog_cols = [col for col in clip_log_df.columns if col.endswith(\"_logClip\")]\n",
    "softClip_cols = [col for col in clip_log_df.columns if col.endswith(\"_softClip\")]\n",
    "\n",
    "check_skew(df)\n",
    "check_skew(clip_log_df[log_cols])\n",
    "# check_skew(clip_log_df[clip_cols])\n",
    "# check_skew(clip_log_df[clipLog_cols])\n",
    "# check_skew(clip_log_df[softClip_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_outliers(df,display =True):\n",
    "    def count_outliers(series, threshold=3):\n",
    "        z_scores = np.abs((series - series.mean()) / series.std())\n",
    "        return (z_scores > threshold).sum()\n",
    "\n",
    "\n",
    "    non_binary_cols = [col for col in df.columns if df[col].nunique() >2]\n",
    "    outliers_count = df[non_binary_cols].apply(count_outliers)\n",
    "\n",
    "    print(\"Number of outliers for each feature:\")\n",
    "    outliers_df = pd.DataFrame(data=outliers_count)\n",
    "    outliers_df[\"total samples\"] = len(df)\n",
    "    outliers_df = outliers_df.sort_values(by=0, ascending=False).rename(columns={0: \"Outliers\"})\n",
    "\n",
    "    # Create a stacked bar plot\n",
    "    outliers_df.plot(kind=\"bar\", y = \"Outliers\",colormap=\"viridis\")\n",
    "\n",
    "    if display == True:\n",
    "    \n",
    "        plt.legend(title=\"Total Samples\", loc=\"upper left\")\n",
    "\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Count of Outliers\")\n",
    "        plt.title(\"Stacked Bar Graph of Outliers by Feature\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return outliers_df\n",
    "\n",
    "detect_outliers(df,False)\n",
    "detect_outliers(clip_log_df[log_cols], False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkify_churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
